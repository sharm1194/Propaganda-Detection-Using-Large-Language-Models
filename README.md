# Propaganda-Detection-Using-Large-Language-Models
This project focuses on two primary detection approaches: binary classification and multiclass classification. Here, popular pre-trained large language models (BERT), A pretrained GloVe (Global Vectors for Word Representation) word embedding model, and LSTM have been utilized. 

I have utilized the mechanism of these models and evaluated their effective- ness in detail. Furthermore, a comparative analysis is conducted between binary and multi-class classification models, considering future improvements. Here, BERT-based Neural Network model outperforms other methods, achieving an accuracy of 73% and F1 score of 74% in binary classification of propaganda detection.
